{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCnCwEOgfSks"
      },
      "source": [
        "# Epsilon-greedy\n",
        "\n",
        "Say, we have two arms - arm 1 and arm 2. Suppose, with arm 1 we win the game 80% of the time and with arm 2 we win the game with 20% of the time. So, we can say that arm 1 is the best arm as it makes us win the game 80% of the time. Now, let's learn how to find this with the epsilon-greedy method.\n",
        "\n",
        "\n",
        "It works with a parameter epsilon (which should be a probability).\n",
        "We select the best arm with probability 1-epsilon and we select the random arm with probability epsilon. Let's take a simple example and learn how we find the best arm exactly with the epsilon-greedy method in more detail. Here, we'll use epsilon = 0.5\n",
        "\n",
        "\n",
        "First, we initialize the `count` - number of times the arm is pulled, `sum_rewards` - the sum of rewards obtained from pulling the arm, `Q`- average reward obtained by pulling the arm\n",
        "\n",
        "## Round 1:\n",
        "\n",
        "Say, in round 1 of the game, we select the random arm with probability epsilon, suppose we randomly pull the arm 1 and observe the reward. Let the reward obtained by pulling the arm 1 be 1. So, we update our table with `count` of arm 1 to 1, `sum_rewards` of arm 1 to 1 and thus the average reward `Q` of the arm 1 after round 1 will be 1\n",
        "\n",
        "## Round 2:\n",
        "\n",
        "Say, in round 2, we select the best arm with probability 1-epsilon. The best arm is the one which has a maximum average reward. So, we check our table as which arm has the maximum average reward, since arm 1 has the maximum average reward, we pull the arm 1 and observe the reward and let the reward obtained from pulling the arm 1 be 1. So, we update our table with `count` of arm 1 to 2, `sum_rewards` of arm 1 to 2 and thus the average reward `Q` of the arm 1 after round 2 will be 1\n",
        "\n",
        "## Round 3:\n",
        "\n",
        "Say, in round 3, we select the random arm with probability epsilon, suppose we randomly pull the arm 2 and observe the reward. Let the reward obtained by pulling the arm 2 be 0. So, we update our table with `count` of arm 2 to 1, `sum_rewards` of arm 2 to 0 and thus the average reward `Q` of the arm 2 after round 3 will be 0\n",
        "\n",
        "## Round 4:\n",
        "\n",
        "Say, in round 4, we select the best arm with probability 1-epsilon. So, we pull arm 1 since it has a maximum average reward. Let the reward obtained by pulling arm 1 be 0 this time. Now, we update our table with `count` of arm 1 to 3, `sum_rewards` of arm 2 to 2 and thus the average reward `Q` of the arm 1 after round 4 will be 0.66\n",
        "\n",
        "\n",
        "## Round n:\n",
        "We repeat this process for several numbers of rounds, that is, for several rounds of the game, we pull the best arm with probability 1-epsilon and we pull the random arm with the probability epsilon. The updated table after some 100 rounds of game\n",
        "\n",
        "\n",
        "We can conclude that arm 1 is the best arm since it has the maximum average reward.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXRsyi0fem2o"
      },
      "source": [
        "# Implementing epsilon-greedy\n",
        "\n",
        "Now, let's learn how to implement the epsilon-greedy method to find the best arm.\n",
        "\n",
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym_bandits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19JkIxkNhLqG",
        "outputId": "65c91816-4e79-4cdc-9f05-a00f9cad5a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym_bandits\n",
            "  Downloading gym_bandits-0.0.1-py3-none-any.whl (3.3 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym_bandits) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gym_bandits) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym_bandits) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym_bandits) (0.0.8)\n",
            "Installing collected packages: gym_bandits\n",
            "Successfully installed gym_bandits-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are using google colab\n",
        "!pip install git+https://github.com/JKCooper2/gym-bandits.git\n",
        "\n",
        "# If you are not using google colab\n",
        "#git clone https://github.com/JKCooper2/gym-bandits.git\n",
        "#cd gym-bandits\n",
        "#pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU23pBoPiUhF",
        "outputId": "71d4fa49-e051-4d40-d981-f8474fbd3182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/JKCooper2/gym-bandits.git\n",
            "  Cloning https://github.com/JKCooper2/gym-bandits.git to /tmp/pip-req-build-sx5v21_x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JKCooper2/gym-bandits.git /tmp/pip-req-build-sx5v21_x\n",
            "  Resolved https://github.com/JKCooper2/gym-bandits.git to commit 417ed323ca2f7298a3abdad34b781fa9f13148f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-bandits==0.0.2) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-nvmN-eem2w"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_bandits\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAhrrq7gem20"
      },
      "source": [
        "## Creating the bandit environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3LSSjfEem22"
      },
      "source": [
        "For better understanding, let's create the bandit with only two arms (as I explained at the begining):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTRVBwoTem22",
        "outputId": "a01c754f-1897-4772-d676-2a561f716a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"BanditTwoArmedHighLowFixed-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0ppGZ-em23"
      },
      "source": [
        "Let's check the probability distribution of the arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQQoaU7kem24",
        "outputId": "15ea037f-cc8f-4cdd-c37b-fbb32b99866e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        }
      ],
      "source": [
        "print(env.p_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdFic3w8em26"
      },
      "source": [
        "We can observe that with arm 1 we win the game with 80% probability and with arm 2 we\n",
        "win the game with 20% probability. Here, the best arm is arm 1, as with arm 1 we win the\n",
        "game 80% probability. Now, let's see how to find this best arm using the epsilon-greedy\n",
        "method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_F-j1kIem27"
      },
      "source": [
        "## Initialize the variables\n",
        "\n",
        "First, let's initialize the variables:\n",
        "\n",
        "Initialize the `count` for storing the number of times an arm is pulled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAeeCHwJem28"
      },
      "outputs": [],
      "source": [
        "count = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVKHlxOnem2-"
      },
      "source": [
        "Initialize the `sum_rewards` for storing the sum of rewards of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlRLt-2zem2_"
      },
      "outputs": [],
      "source": [
        "sum_rewards = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If4iwvo4em3A"
      },
      "source": [
        "Initialize the `Q` for storing the average reward of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBgtts_Nem3A"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x38398o8em3A"
      },
      "source": [
        "Define `num_rounds` - number of rounds (iterations):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "MDStemovem3B"
      },
      "outputs": [],
      "source": [
        "num_rounds = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZlKsyTQem3B"
      },
      "source": [
        "## Defining the epsilon-greedy method\n",
        "\n",
        "First, we generate a random number from a uniform distribution, if the random number is\n",
        "less than epsilon then pull the random arm else we pull the best arm which has maximum\n",
        "average reward as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5vDeQKQem3C"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(epsilon):\n",
        "\n",
        "    if np.random.uniform(0,1) < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2buMDXapem3C"
      },
      "source": [
        "## Start pulling the arm\n",
        "\n",
        "Now, let's play the game and try to find the best arm using the epsilon-greedy method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "x8PGYgBsem3D"
      },
      "outputs": [],
      "source": [
        "# The gym_bandits environment requires a call to env.reset()\n",
        "# before we can make the first env.step()\n",
        "env.reset()\n",
        "\n",
        "# Now we can start the game\n",
        "for i in range(num_rounds):\n",
        "\n",
        "    #select the arm based on the epsilon-greedy method\n",
        "    arm = epsilon_greedy(0.5)\n",
        "\n",
        "    #pull the arm and store the reward and next state information\n",
        "    next_state, reward, done, info = env.step(arm)\n",
        "\n",
        "    #increment the count of the arm by 1\n",
        "    count[arm] += 1\n",
        "\n",
        "    #update the sum of rewards of the arm\n",
        "    sum_rewards[arm]+=reward\n",
        "\n",
        "    #update the average reward of the arm\n",
        "    Q[arm] = sum_rewards[arm]/count[arm]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg5INCMaem3E"
      },
      "source": [
        "After all the rounds, we look at the average reward obtained from each of the arms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFPMga2Zem3E",
        "outputId": "047b2790-a382-499f-b0f5-d64938d078ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75694444 0.19642857]\n"
          ]
        }
      ],
      "source": [
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvQzyQtLem3F"
      },
      "source": [
        "Now, we can select the optimal arm as the one which has a maximum average reward. Since the arm 1 has a maximum average reward than the arm 2, our optimal arm will be\n",
        "arm 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgkgNM0Zem3F",
        "outputId": "7785ba8a-d5cd-4809-f6ee-a8db7b0d1f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal arm is arm 1\n"
          ]
        }
      ],
      "source": [
        "print('The optimal arm is arm {}'.format(np.argmax(Q)+1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}