{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Thompson sampling\n",
        "\n",
        "Thompson sampling is a strategy used in reinforcement learning to find the best actions while exploring the environment. Instead of using fixed rules, Thompson sampling uses probability distributions to represent its belief about the rewards of different actions.\n",
        "\n",
        "In Thompson sampling, the agent maintains these probability distributions and samples from them to select actions. The higher the probability of an action being the best, the more likely it is to be chosen. This allows for a balance between exploring new actions and exploiting the actions that seem to have higher rewards.\n",
        "\n",
        "The key idea behind Thompson sampling is to embrace uncertainty. By sampling from the probability distributions and selecting the action with the highest sampled reward, the strategy naturally explores actions that have high potential rewards but are uncertain. It also exploits actions with lower uncertainty and lower estimated rewards.\n",
        "\n",
        "Thompson sampling has advantages such as not requiring manual adjustment of exploration parameters. The exploration behavior is automatically determined by the probabilistic sampling. It also has strong theoretical foundations and has been successful in various scenarios.\n",
        "\n",
        "However, Thompson sampling can be computationally demanding compared to simpler strategies like epsilon-greedy or UCB. It involves maintaining and updating the probability distributions, which can be resource-intensive."
      ],
      "metadata": {
        "id": "lAdFg7ImCBsi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOK8WVgYAoMZ"
      },
      "source": [
        "# Implementing Thompson sampling\n",
        "\n",
        "Now, let's learn how to implement the Thompson sampling method to find the best arm.\n",
        "\n",
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are using google colab\n",
        "!pip install git+https://github.com/JKCooper2/gym-bandits.git\n",
        "\n",
        "# If you are not using google colab\n",
        "#git clone https://github.com/JKCooper2/gym-bandits.git\n",
        "#cd gym-bandits\n",
        "#pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J16Qcp7CA_W6",
        "outputId": "53cd3b7b-3190-4610-fdae-01e1fc47a621"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/JKCooper2/gym-bandits.git\n",
            "  Cloning https://github.com/JKCooper2/gym-bandits.git to /tmp/pip-req-build-ngblyjv4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JKCooper2/gym-bandits.git /tmp/pip-req-build-ngblyjv4\n",
            "  Resolved https://github.com/JKCooper2/gym-bandits.git to commit 417ed323ca2f7298a3abdad34b781fa9f13148f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-bandits==0.0.2) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (0.0.8)\n",
            "Building wheels for collected packages: gym-bandits\n",
            "  Building wheel for gym-bandits (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-bandits: filename=gym_bandits-0.0.2-py3-none-any.whl size=5176 sha256=a0af5bff1fd4d5cec7a9d30658b10e764a1b3dbaf02a1d4049782d84e966880f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ff56z1dl/wheels/2e/94/6b/ee0d6aafd6f5273960cc3127123c3a09681b4becdabc1b1893\n",
            "Successfully built gym-bandits\n",
            "Installing collected packages: gym-bandits\n",
            "Successfully installed gym-bandits-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "efLwN73CAoMd"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_bandits\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSTCcrXVAoMg"
      },
      "source": [
        "## Creating the bandit environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UXs7qTRAoMh"
      },
      "source": [
        "Let's take the same two-armed bandit we saw in the previous section:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUQOloNmAoMj",
        "outputId": "4f1a56e1-d58a-418e-bd07-c1f3092679e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"BanditTwoArmedHighLowFixed-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGegVrFDAoMl"
      },
      "source": [
        "Let's check the probability distribution of the arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCasI_qcAoMm",
        "outputId": "e6a1b132-ffb4-4f2d-d022-02378a18f829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        }
      ],
      "source": [
        "print(env.p_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGY0IAESAoMn"
      },
      "source": [
        "We can observe that with arm 1 we win the game with 80% probability and with arm 2 we\n",
        "win the game with 20% probability. Here, the best arm is arm 1, as with arm 1 we win the\n",
        "game 80% probability. Now, let's see how to find this best arm using the thompson sampling method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqsg0nl_AoMo"
      },
      "source": [
        "## Initialize the variables\n",
        "\n",
        "First, let's initialize the variables:\n",
        "\n",
        "Initialize the `count` for storing the number of times an arm is pulled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HcQlpOoIAoMp"
      },
      "outputs": [],
      "source": [
        "count = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xCbB7aSAoMq"
      },
      "source": [
        "Initialize the `sum_rewards` for storing the sum of rewards of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UBQ9ACMdAoMr"
      },
      "outputs": [],
      "source": [
        "sum_rewards = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GoqU7zrAoMr"
      },
      "source": [
        "Initialize the `Q` for storing the average reward of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dKeJKTJjAoMs"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lhMGPtOAoMs"
      },
      "source": [
        "Define `num_rounds` - number of rounds (iterations):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": false,
        "id": "wFWUb3I5AoMs"
      },
      "outputs": [],
      "source": [
        "num_rounds = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11AtxrCAoMt"
      },
      "source": [
        "Initialize the `alpha` value with 1 for both the arms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6zT3y6sLAoMu"
      },
      "outputs": [],
      "source": [
        "alpha = np.ones(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNmJr1pVAoMu"
      },
      "source": [
        "Initialize the `beta` value with 1 for both the arms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rDP1wWP8AoMv"
      },
      "outputs": [],
      "source": [
        "beta = np.ones(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlbVwgJkAoMv"
      },
      "source": [
        "## Defining the Thompson Sampling function\n",
        "\n",
        "Now, let's define the `thompson_sampling` function.\n",
        "\n",
        "As shown below, we randomly sample value from the beta distribution of both the arms\n",
        "and return the arm which has the maximum sampled value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Iizw-j2UAoMw"
      },
      "outputs": [],
      "source": [
        "def thompson_sampling(alpha,beta):\n",
        "\n",
        "    samples = [np.random.beta(alpha[i]+1,beta[i]+1) for i in range(2)]\n",
        "\n",
        "    return np.argmax(samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlPZkTBoAoMw"
      },
      "source": [
        "## Start pulling the arm\n",
        "\n",
        "Now, let's play the game and try to find the best arm using the Thompson sampling\n",
        "method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBM-Ia9dAoMy",
        "outputId": "60dc7c61-a4e3-49aa-bcf8-3103342310df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "# The gym_bandits environment requires a call to env.reset()\n",
        "# before we can make the first env.step()\n",
        "env.reset()\n",
        "\n",
        "# Now we can start the game\n",
        "for i in range(num_rounds):\n",
        "\n",
        "    #select the arm based on the thompson sampling method\n",
        "    arm = thompson_sampling(alpha,beta)\n",
        "\n",
        "    #pull the arm and store the reward and next state information\n",
        "    next_state, reward, done, info = env.step(arm)\n",
        "\n",
        "    #increment the count of the arm by 1\n",
        "    count[arm] += 1\n",
        "\n",
        "    #update the sum of rewards of the arm\n",
        "    sum_rewards[arm]+=reward\n",
        "\n",
        "    #update the average reward of the arm\n",
        "    Q[arm] = sum_rewards[arm]/count[arm]\n",
        "\n",
        "    #if we win the game, that is, if the reward is equal to 1, then we update the value of alpha as\n",
        "    #alpha = alpha + 1 else we update the value of beta as beta = beta + 1\n",
        "    if reward==1:\n",
        "        alpha[arm] = alpha[arm] + 1\n",
        "    else:\n",
        "        beta[arm] = beta[arm] + 1\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSbkEK6KAoMy"
      },
      "source": [
        "After all the rounds, we look at the average reward obtained from each of the arms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T0j82YZAoMz",
        "outputId": "57f08295-62ad-454f-96a6-6cb464092d2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.77173913 0.5       ]\n"
          ]
        }
      ],
      "source": [
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPm4dzPnAoMz"
      },
      "source": [
        "Now, we can select the optimal arm as the one which has a maximum average reward. Since the arm 1 has a maximum average reward than the arm 2, our optimal arm will be\n",
        "arm 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aijAy6z5AoMz",
        "outputId": "07d97588-9f0c-49d1-ec44-7a7f9235ce45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal arm is arm 1\n"
          ]
        }
      ],
      "source": [
        "print('The optimal arm is arm {}'.format(np.argmax(Q)+1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}