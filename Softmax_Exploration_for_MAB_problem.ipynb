{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax Exploration\n",
        "\n",
        "The softmax exploration method is another approach to balance exploration and exploitation. Instead of selecting actions completely randomly as in epsilon-greedy, softmax assigns a probability distribution over all possible actions based on their estimated values.\n",
        "\n",
        "In the softmax method, each action's selection probability is calculated using the softmax function, which converts action values into probabilities. The softmax function gives higher probability to actions with higher estimated values, but it still assigns non-zero probabilities to all actions, allowing for exploration.\n",
        "\n",
        "For example, let’s assume we have 4 arms and arm 1 is the best arm. After exploring the non-best arms – [arm 2, arm 3, arm 4] – uniformly, we realised that arm 3 is never a good arm and it always gives a reward of 0. In this case, instead of exploring arm 3 again, we can spend more time exploring arm 2 and arm 4. But the problem with the epsilon-greedy method is that we explore all the non-best arms equally. So, all the non-best arms – [arm 2, arm 3, arm 4] – will be explored equally. To avoid this, if we can give priority to arm 2 and arm 4 over arm 3, then we can explore arm 2 and arm 4 more than arm 3. We can give priority to the arms by assigning a probability to all the arms based on the average reward Q. The arm that has the maximum average reward will have high probability, and all the non-best arms have a probability proportional to their average reward.\n",
        "\n",
        "The degree of exploration in the softmax method is controlled by a temperature parameter. Higher temperature values make the action probabilities more uniform, leading to increased exploration, while lower temperature values make the action probabilities sharper and closer to deterministic, favoring exploitation.\n",
        "\n",
        "Softmax exploration provides a more continuous and controlled exploration behavior compared to epsilon-greedy. It can be beneficial when the agent needs to explore the action space more systematically or when there are multiple actions with similar values that need to be explored further.\n",
        "\n"
      ],
      "metadata": {
        "id": "Aw6m5HLt-epn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt7IHdgTmgPc"
      },
      "source": [
        "# Implementing Softmax Exploration\n",
        "\n",
        "Now, let's learn how to implement the softmax exploration to find the best arm.\n",
        "\n",
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are using google colab\n",
        "!pip install git+https://github.com/JKCooper2/gym-bandits.git\n",
        "\n",
        "# If you are not using google colab\n",
        "#git clone https://github.com/JKCooper2/gym-bandits.git\n",
        "#cd gym-bandits\n",
        "#pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjkgHr7XnwJD",
        "outputId": "b9850b97-2bb1-4fcf-ce0c-504263a0bcdf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/JKCooper2/gym-bandits.git\n",
            "  Cloning https://github.com/JKCooper2/gym-bandits.git to /tmp/pip-req-build-azo3slsn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JKCooper2/gym-bandits.git /tmp/pip-req-build-azo3slsn\n",
            "  Resolved https://github.com/JKCooper2/gym-bandits.git to commit 417ed323ca2f7298a3abdad34b781fa9f13148f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-bandits==0.0.2) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "G532Pr_emgPg"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_bandits\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioY8csVnmgPj"
      },
      "source": [
        "## Creating the bandit environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY3CZK_RmgPl"
      },
      "source": [
        "Let's take the same two-armed bandit we saw in the epsilon-greedy section:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sOOBqwJTmgPm"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"BanditTwoArmedHighLowFixed-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kALAI7EgmgPp"
      },
      "source": [
        "Let's check the probability distribution of the arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueANY5fXmgPr",
        "outputId": "02f92eae-c7a0-4b93-fe34-f5c8cd06b5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        }
      ],
      "source": [
        "print(env.p_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRXO2Yp4mgPs"
      },
      "source": [
        "We can observe that with arm 1 we win the game with 80% probability and with arm 2 we\n",
        "win the game with 20% probability. Here, the best arm is arm 1, as with arm 1 we win the\n",
        "game 80% probability. Now, let's see how to find this best arm using the softmax exploration method\n",
        "method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v_Yqa36mgPu"
      },
      "source": [
        "## Initialize the variables\n",
        "\n",
        "First, let's initialize the variables:\n",
        "\n",
        "Initialize the `count` for storing the number of times an arm is pulled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZT5AIvN7mgPw"
      },
      "outputs": [],
      "source": [
        "count = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65f2370RmgPx"
      },
      "source": [
        "Initialize the `sum_rewards` for storing the sum of rewards of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "rlq6IjY2mgPx"
      },
      "outputs": [],
      "source": [
        "sum_rewards = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b_SLLDKmgPy"
      },
      "source": [
        "Initialize the `Q` for storing the average reward of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "7zqJeraXmgPz"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHBiYOPimgPz"
      },
      "source": [
        "Define `num_rounds` - number of rounds (iterations):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "scrolled": false,
        "id": "dBO33-B7mgP0"
      },
      "outputs": [],
      "source": [
        "num_rounds = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnvWjN_MmgP0"
      },
      "source": [
        "## Defining the softmax exploration function\n",
        "\n",
        "Now, let's define the softmax function with temperature `T` as:\n",
        "\n",
        "$$P_t(a) = \\frac{\\text{exp}(Q_t(a)/T)} {\\sum_{i=1}^n \\text{exp}(Q_t(i)/T)} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EweJfK99mgP1"
      },
      "outputs": [],
      "source": [
        "def softmax(T):\n",
        "\n",
        "    #compute the probability of each arm based on the above equation\n",
        "    denom = sum([np.exp(i/T) for i in Q])\n",
        "    probs = [np.exp(i/T)/denom for i in Q]\n",
        "\n",
        "    #select the arm based on the computed probability distribution of arms\n",
        "    arm = np.random.choice(env.action_space.n, p=probs)\n",
        "\n",
        "    return arm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00uTY6i2mgP2"
      },
      "source": [
        "## Start pulling the arm\n",
        "\n",
        "Now, let's play the game and try to find the best arm using the softmax exploration method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4aVkfUxmgP2"
      },
      "source": [
        "Let's begin by setting the temperature `T` to a high number, say 50:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "p7ecCzIwmgP3"
      },
      "outputs": [],
      "source": [
        "T = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "id": "4mp-rttxmgP3"
      },
      "outputs": [],
      "source": [
        "# The gym_bandits environment requires a call to env.reset()\n",
        "# before we can make the first env.step()\n",
        "env.reset()\n",
        "\n",
        "# Now we can start the game\n",
        "for i in range(num_rounds):\n",
        "\n",
        "    #select the arm based on the softmax exploration method\n",
        "    arm = softmax(T)\n",
        "\n",
        "    #pull the arm and store the reward and next state information\n",
        "    next_state, reward, done, info = env.step(arm)\n",
        "\n",
        "    #increment the count of the arm by 1\n",
        "    count[arm] += 1\n",
        "\n",
        "    #update the sum of rewards of the arm\n",
        "    sum_rewards[arm]+=reward\n",
        "\n",
        "    #update the average reward of the arm\n",
        "    Q[arm] = sum_rewards[arm]/count[arm]\n",
        "\n",
        "    #reduce the temperature\n",
        "    T = T*0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4Y3kkqWmgP4"
      },
      "source": [
        "After all the rounds, we look at the average reward obtained from each of the arms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ndy597hmgP5",
        "outputId": "8eb08ce2-bbab-450b-dd97-9d308ed7ee49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.75510204 0.17647059]\n"
          ]
        }
      ],
      "source": [
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__g1bsO6mgP5"
      },
      "source": [
        "Now, we can select the optimal arm as the one which has a maximum average reward. Since the arm 1 has a maximum average reward than the arm 2, our optimal arm will be\n",
        "arm 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9fYcy-XmgP6",
        "outputId": "0970b83f-6b1a-4179-cfcd-8407f88c8803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal arm is arm 1\n"
          ]
        }
      ],
      "source": [
        "print('The optimal arm is arm {}'.format(np.argmax(Q)+1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}