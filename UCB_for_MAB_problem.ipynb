{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The UCB (Upper Confidence Bound)\n",
        "\n",
        "The UCB (Upper Confidence Bound) algorithm is another exploration strategy commonly used in reinforcement learning. Unlike epsilon-greedy and softmax methods, UCB takes into account not only the estimated values of actions but also the uncertainty or confidence in those estimates.\n",
        "\n",
        "The UCB algorithm aims to balance exploration and exploitation by selecting actions based on an upper confidence bound, which is a measure of the potential upper limit of an action's value. The idea is to prioritize actions that have higher estimated values but also higher uncertainty.\n",
        "\n",
        "The UCB algorithm prioritizes actions that have high estimated values but have been selected fewer times, thus promoting exploration of potentially promising actions. As the number of times an action is selected increases, the uncertainty decreases, and the algorithm tends to exploit the actions with higher estimated values.\n",
        "\n",
        "The UCB algorithm is known for its theoretical guarantees and its ability to converge to the optimal action with fewer samples compared to some other exploration methods. However, it does require a prior estimation of the confidence bounds, and it may not perform optimally in all scenarios or when the underlying environment dynamics change over time.\n"
      ],
      "metadata": {
        "id": "l2KbGVNi_9Qp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAOP3lav_S9t"
      },
      "source": [
        "# Implementing UCB\n",
        "\n",
        "Now, let's learn how to implement the UCB algorithm to find the best arm.\n",
        "\n",
        "First, let's import the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are using google colab\n",
        "!pip install git+https://github.com/JKCooper2/gym-bandits.git\n",
        "\n",
        "# If you are not using google colab\n",
        "#git clone https://github.com/JKCooper2/gym-bandits.git\n",
        "#cd gym-bandits\n",
        "#pip install -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrZgKaj5AKGu",
        "outputId": "afc46132-73f2-486d-dbd3-20469f56afab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/JKCooper2/gym-bandits.git\n",
            "  Cloning https://github.com/JKCooper2/gym-bandits.git to /tmp/pip-req-build-38ofgjvo\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JKCooper2/gym-bandits.git /tmp/pip-req-build-38ofgjvo\n",
            "  Resolved https://github.com/JKCooper2/gym-bandits.git to commit 417ed323ca2f7298a3abdad34b781fa9f13148f1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from gym-bandits==0.0.2) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->gym-bandits==0.0.2) (0.0.8)\n",
            "Building wheels for collected packages: gym-bandits\n",
            "  Building wheel for gym-bandits (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-bandits: filename=gym_bandits-0.0.2-py3-none-any.whl size=5176 sha256=6c7565fbf265e218484eaa2fe857bb999979d9c7b35bd844055fa1e19141cde6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l87yq6kx/wheels/2e/94/6b/ee0d6aafd6f5273960cc3127123c3a09681b4becdabc1b1893\n",
            "Successfully built gym-bandits\n",
            "Installing collected packages: gym-bandits\n",
            "Successfully installed gym-bandits-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rq5t5CeH_S9y"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import gym_bandits\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8hAzAVN_S91"
      },
      "source": [
        "## Creating the bandit environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoIeJiEL_S91"
      },
      "source": [
        "Let's take the same two-armed bandit we saw in the epsilon-greedy section:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHtGZIde_S92",
        "outputId": "3f88253a-f941-479e-8d70-99a75ce143b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"BanditTwoArmedHighLowFixed-v0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0M0NXlz_S94"
      },
      "source": [
        "Let's check the probability distribution of the arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J--PsN8k_S95",
        "outputId": "65d40426-3274-460c-bda5-323d855a5755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8, 0.2]\n"
          ]
        }
      ],
      "source": [
        "print(env.p_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohfwjd00_S97"
      },
      "source": [
        "We can observe that with arm 1 we win the game with 80% probability and with arm 2 we\n",
        "win the game with 20% probability. Here, the best arm is arm 1, as with arm 1 we win the\n",
        "game 80% probability. Now, let's see how to find this best arm using the UCB method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1WgVa6_S98"
      },
      "source": [
        "## Initialize the variables\n",
        "\n",
        "First, let's initialize the variables:\n",
        "\n",
        "Initialize the `count` for storing the number of times an arm is pulled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4mTpjUaV_S99"
      },
      "outputs": [],
      "source": [
        "count = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgmSHon4_S9-"
      },
      "source": [
        "Initialize the `sum_rewards` for storing the sum of rewards of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3D0gKuc1_S9_"
      },
      "outputs": [],
      "source": [
        "sum_rewards = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwieOhAr_S-A"
      },
      "source": [
        "Initialize `Q` for storing the average reward of each arm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YrkPrBh5_S-A"
      },
      "outputs": [],
      "source": [
        "Q = np.zeros(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwaDQt5l_S-A"
      },
      "source": [
        "Define `num_rounds` number of rounds (iterations):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": false,
        "id": "Lfst6S_z_S-B"
      },
      "outputs": [],
      "source": [
        "num_rounds = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZMooOaK_S-B"
      },
      "source": [
        "## Defining the UCB function\n",
        "\n",
        "Now, we define the `UCB` function which returns the best arm as the one which has the\n",
        "high upper confidence bound (UCB) arm:\n",
        "\n",
        "$$ \\text{UCB(a)} =Q(a) +\\sqrt{\\frac{2 \\log(t)}{N(a)}}  --- (1) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LZVVXVO__S-H"
      },
      "outputs": [],
      "source": [
        "def UCB(i):\n",
        "\n",
        "    #initialize the numpy array for storing the UCB of all the arms\n",
        "    ucb = np.zeros(2)\n",
        "\n",
        "    #before computing the UCB, we explore all the arms at least once, so for the first 2 rounds,\n",
        "    #we directly select the arm corresponding to the round number\n",
        "    if i < 2:\n",
        "        return i\n",
        "\n",
        "    #if the round is greater than 10 then, we compute the UCB of all the arms as specified in the\n",
        "    #equation (1) and return the arm which has the highest UCB:\n",
        "    else:\n",
        "        for arm in range(2):\n",
        "            ucb[arm] = Q[arm] + np.sqrt((2*np.log(sum(count))) / count[arm])\n",
        "        return (np.argmax(ucb))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dTZzf-K_S-I"
      },
      "source": [
        "## Start pulling the arm\n",
        "\n",
        "Now, let's play the game and try to find the best arm using the UCB method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apcwOqWH_S-J",
        "outputId": "9e2d5574-6d0c-4c73-e26b-0b9fd1b74168"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ],
      "source": [
        "# The gym_bandits environment requires a call to env.reset()\n",
        "# before we can make the first env.step()\n",
        "env.reset()\n",
        "\n",
        "# Now we can start the game\n",
        "for i in range(num_rounds):\n",
        "\n",
        "    #select the arm based on the UCB method\n",
        "    arm = UCB(i)\n",
        "\n",
        "    #pull the arm and store the reward and next state information\n",
        "    next_state, reward, done, info = env.step(arm)\n",
        "\n",
        "    #increment the count of the arm by 1\n",
        "    count[arm] += 1\n",
        "\n",
        "    #update the sum of rewards of the arm\n",
        "    sum_rewards[arm]+=reward\n",
        "\n",
        "    #update the average reward of the arm\n",
        "    Q[arm] = sum_rewards[arm]/count[arm]\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCDn089C_S-J"
      },
      "source": [
        "After all the rounds, we look at the average reward obtained from each of the arms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zphTlvRZ_S-K",
        "outputId": "124adff1-f93c-4fc5-ebcb-80d2ba74cfe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.85227273 0.25      ]\n"
          ]
        }
      ],
      "source": [
        "print(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMYJMADr_S-K"
      },
      "source": [
        "Now, we can select the optimal arm as the one which has a maximum average reward. Since the arm 1 has a maximum average reward than the arm 2, our optimal arm will be\n",
        "arm 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXrcL-yl_S-L",
        "outputId": "3830b095-44a6-42f4-e9db-d82dab4134dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal arm is arm 1\n"
          ]
        }
      ],
      "source": [
        "print('The optimal arm is arm {}'.format(np.argmax(Q)+1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}